{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386a8faa",
   "metadata": {},
   "source": [
    "### Mission\n",
    "\n",
    "We recently used Naive Bayes to classify spam in this [SMSSpamClassifier dataset](http://localhost:8888/edit/Notebooks/Supervised%20Learning/Ensemble%20Learning/data/SMSSpamCollection). \\\n",
    "In this notebook, we will expand on the previous analysis by using a few of the new techniques we've learned in Ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1a414",
   "metadata": {},
   "source": [
    "First we will recreate the Naive Bayes model and then use the Ensemble techniques on the same dataset and compare the 2 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cf233",
   "metadata": {},
   "source": [
    "#### 1. Recreate the Naive Bayes model on the Spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b57887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9885139985642498\n",
      "Precision score:  0.9720670391061452\n",
      "Recall score:  0.9405405405405406\n",
      "F1 score:  0.9560439560439562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Read in our dataset\n",
    "df = pd.read_table('./data/SMSSpamCollection', sep = '\\t', names = ['label', 'sms_message'])\n",
    "\n",
    "# Fix our response value\n",
    "df['label'] = df.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Split our dataset into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, -1].values, \n",
    "                                                    df.iloc[:, :-1].values, \n",
    "                                                    random_state = 1)\n",
    "\n",
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)\n",
    "\n",
    "# Instantiate our model\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Fit our model to the training data\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = naive_bayes.predict(testing_data)\n",
    "\n",
    "# Score our model\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9eaf6",
   "metadata": {},
   "source": [
    "#### 2. Now try different Ensemble techniques and compare the models \n",
    "\n",
    "We can see from the scores above that our Naive Bayes model actually does a pretty good job of classifying \"spam\" and \"ham.\"  However, let's take a look at a few additional models to see if we can't improve anyway.\n",
    "\n",
    "Specifically, we will take a look at the following techniques:\n",
    "\n",
    "* [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "* [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "* [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "\n",
    "Really useful guide for ensemble methods can be found [in the documentation here](http://scikit-learn.org/stable/modules/ensemble.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ce65d",
   "metadata": {},
   "source": [
    "Since we have cleaned and vectorized the text as per above, we now can be focused on - the machine learning part.\n",
    "\n",
    "In general, there is a five step process that can be used each time you want to use a supervised learning method (which we have used):\n",
    "\n",
    "1. **Import** the model.\n",
    "2. **Instantiate** the model with the hyperparameters of interest.\n",
    "3. **Fit** the model to the training data.\n",
    "4. **Predict** on the test data.\n",
    "5. **Score** the model by comparing the predictions to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fc7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Bagging, RandomForest, and AdaBoost Classifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa492e2b",
   "metadata": {},
   "source": [
    "Now instantiate each of the classifiers with the hyperparameters mentioned on each comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86dbf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a BaggingClassifier with:\n",
    "# 200 weak learners (n_estimators) and everything else as default values\n",
    "\n",
    "bag_classifier = BaggingClassifier(n_estimators = 200)\n",
    "\n",
    "\n",
    "# Instantiate a RandomForestClassifier with:\n",
    "# 200 weak learners (n_estimators) and everything else as default values\n",
    "\n",
    "random_classifier = RandomForestClassifier(n_estimators = 200)\n",
    "\n",
    "\n",
    "# Instantiate an a AdaBoostClassifier with:\n",
    "# With 300 weak learners (n_estimators) and a learning_rate of 0.2\n",
    "\n",
    "ada_classifier = AdaBoostClassifier(n_estimators = 300, learning_rate = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff828fa",
   "metadata": {},
   "source": [
    "Fit each model using the `training_data` and `y_train` obtained above in the Naive Bayes section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b16cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "<ipython-input-4-c50563df34f3>:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  random_classifier.fit(training_data, y_train)\n",
      "/root/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.2, n_estimators=300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit your BaggingClassifier to the training data\n",
    "\n",
    "bag_classifier.fit(training_data, y_train)\n",
    "\n",
    "\n",
    "# Fit your RandomForestClassifier to the training data\n",
    "\n",
    "random_classifier.fit(training_data, y_train)\n",
    "\n",
    "\n",
    "# Fit your AdaBoostClassifier to the training data\n",
    "\n",
    "ada_classifier.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfafa0a",
   "metadata": {},
   "source": [
    "Its time to get the prediction for the above models using the `testing_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ee9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using BaggingClassifier on the test data\n",
    "\n",
    "y_pred_bag = bag_classifier.predict(testing_data)\n",
    "\n",
    "\n",
    "# Predict using RandomForestClassifier on the test data\n",
    "\n",
    "y_pred_randforest = random_classifier.predict(testing_data)\n",
    "\n",
    "\n",
    "# Predict using AdaBoostClassifier on the test data\n",
    "\n",
    "y_pred_ada = ada_classifier.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29943d62",
   "metadata": {},
   "source": [
    "Define a function which will return the score values for our models. \\\n",
    "We will also use this on Naive Bayes to compare the performance side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c68d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, preds, model_name = None):\n",
    "    '''\n",
    "    INPUT:\n",
    "    y_true - the y values that are actually true in the dataset (NumPy array or pandas series)\n",
    "    preds - the predictions for those values from some model (NumPy array or pandas series)\n",
    "    model_name - (str - optional) a name associated with the model if you would like to add it to the print statements \n",
    "    \n",
    "    OUTPUT:\n",
    "    None - prints the accuracy, precision, recall, and F1 score\n",
    "    '''\n",
    "    if model_name == None:\n",
    "        print('Accuracy score: ', format(accuracy_score(y_true, preds)))\n",
    "        print('Precision score: ', format(precision_score(y_true, preds)))\n",
    "        print('Recall score: ', format(recall_score(y_true, preds)))\n",
    "        print('F1 score: ', format(f1_score(y_true, preds)))\n",
    "        print('\\n\\n')\n",
    "    \n",
    "    else:\n",
    "        print('Accuracy score for ' + model_name + ' :' , format(accuracy_score(y_true, preds)))\n",
    "        print('Precision score ' + model_name + ' :', format(precision_score(y_true, preds)))\n",
    "        print('Recall score ' + model_name + ' :', format(recall_score(y_true, preds)))\n",
    "        print('F1 score ' + model_name + ' :', format(f1_score(y_true, preds)))\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5f495",
   "metadata": {},
   "source": [
    "Now print the metrics for individual models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e448282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for BaggingClassifier : 0.9741564967695621\n",
      "Precision score BaggingClassifier : 0.9116022099447514\n",
      "Recall score BaggingClassifier : 0.8918918918918919\n",
      "F1 score BaggingClassifier : 0.9016393442622951\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score for RandomForestsClassifier : 0.9834888729361091\n",
      "Precision score RandomForestsClassifier : 1.0\n",
      "Recall score RandomForestsClassifier : 0.8756756756756757\n",
      "F1 score RandomForestsClassifier : 0.9337175792507205\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score for AdaboostClassifier : 0.9770279971284996\n",
      "Precision score AdaboostClassifier : 0.9693251533742331\n",
      "Recall score AdaboostClassifier : 0.8540540540540541\n",
      "F1 score AdaboostClassifier : 0.9080459770114943\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score for NaiveBayesClassifier : 0.9885139985642498\n",
      "Precision score NaiveBayesClassifier : 0.9720670391061452\n",
      "Recall score NaiveBayesClassifier : 0.9405405405405406\n",
      "F1 score NaiveBayesClassifier : 0.9560439560439562\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Bagging scores\n",
    "\n",
    "print_metrics(y_test, y_pred_bag, 'BaggingClassifier')\n",
    "\n",
    "\n",
    "# Print Random Forest scores\n",
    "\n",
    "print_metrics(y_test, y_pred_randforest, 'RandomForestsClassifier')\n",
    "\n",
    "\n",
    "# Print AdaBoost scores\n",
    "\n",
    "print_metrics(y_test, y_pred_ada, 'AdaboostClassifier')\n",
    "\n",
    "\n",
    "# Naive Bayes Classifier scores\n",
    "\n",
    "print_metrics(y_test, predictions, 'NaiveBayesClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c416772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
